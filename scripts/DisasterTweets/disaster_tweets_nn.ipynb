{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Neural Network model to classify disaster tweets for Kaggle competition. \n",
    "\n",
    "1 Building the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting functions\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "# 2 layer nn definition\n",
    "class NeuralNetwork:\n",
    "    # nn consists of the following components:\n",
    "    # arbitraty amount of hidden layers, 2 in this case\n",
    "    def __init__(self, x, y):\n",
    "        # an input layer, x\n",
    "        self.input      = x\n",
    "        # a set of weights and biases (assumed 0 for simplicity)\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1) \n",
    "        # an output layer, y\n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(y.shape)\n",
    "    \n",
    "    #calculating the predicted output y hat\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "    \n",
    "    # determine the gradient of loss function so we know which direction to move our predictions in\n",
    "    def backprop(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
    "\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:  [[0.00928497]\n",
      " [0.97189493]\n",
      " [0.97234052]\n",
      " [0.03196776]]\n",
      "average error:  [0.00066577]\n"
     ]
    }
   ],
   "source": [
    "# example application\n",
    "if __name__ == \"__main__\":\n",
    "    X = np.array([[0,0,1],\n",
    "                  [0,1,1],\n",
    "                  [1,0,1],\n",
    "                  [1,1,1]])\n",
    "    y = np.array([[0],[1],[1],[0]])\n",
    "    nn = NeuralNetwork(X,y)\n",
    "\n",
    "    for i in range(1500):\n",
    "        nn.feedforward()\n",
    "        nn.backprop()\n",
    "\n",
    "    print(\"predictions: \",nn.output)\n",
    "    print(\"average error: \",sum((nn.output - y)**2)/len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Kaggle competiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(r'nlp-getting-started\\train_clean_spellcheck.csv', usecols = ['id','text','target'])\n",
    "ids = tweets['id']\n",
    "tweets = tweets[['target','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake may Allah forgive us all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la range asks Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are being notified by officers no other evacuation or shelter in place orders are expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orders in California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby Alaska as smoke from wildfires pours into a school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>rocky fire update California hwy 20 closed in both directions due to lake county fire afire wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>flood disaster heavy rain causes flash flooding of streets in mangetout Colorado springs areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>mi on top of the hill and i can see a fire in the woods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>theirs an emergency evacuation happening now in the building across the street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>mi afraid that the tornado is coming to our area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>three people died from the heat wave so far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>aha south Tampa is getting flooded hag wait a second i live in south Tampa what am i gonna do what am i gonna do flick flooding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>raining flooding Florida tamper Tampa 18 or 19 days vie lost count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>flood in boga Myanmar we arrived boga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>damage to school bus on 80 in multi car crash breaking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>whats up man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>i love fruits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>summer is lovely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>my car is so fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>what a goooooooaaaaaal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    target  \\\n",
       "0   1        \n",
       "1   1        \n",
       "2   1        \n",
       "3   1        \n",
       "4   1        \n",
       "5   1        \n",
       "6   1        \n",
       "7   1        \n",
       "8   1        \n",
       "9   1        \n",
       "10  1        \n",
       "11  1        \n",
       "12  1        \n",
       "13  1        \n",
       "14  1        \n",
       "15  0        \n",
       "16  0        \n",
       "17  0        \n",
       "18  0        \n",
       "19  0        \n",
       "\n",
       "                                                                                                                                  text  \n",
       "0   our deeds are the reason of this earthquake may Allah forgive us all                                                                \n",
       "1   forest fire near la range asks Canada                                                                                               \n",
       "2   all residents asked to shelter in place are being notified by officers no other evacuation or shelter in place orders are expected  \n",
       "3   13000 people receive wildfires evacuation orders in California                                                                      \n",
       "4   just got sent this photo from ruby Alaska as smoke from wildfires pours into a school                                               \n",
       "5   rocky fire update California hwy 20 closed in both directions due to lake county fire afire wildfires                               \n",
       "6   flood disaster heavy rain causes flash flooding of streets in mangetout Colorado springs areas                                      \n",
       "7   mi on top of the hill and i can see a fire in the woods                                                                             \n",
       "8   theirs an emergency evacuation happening now in the building across the street                                                      \n",
       "9   mi afraid that the tornado is coming to our area                                                                                    \n",
       "10  three people died from the heat wave so far                                                                                         \n",
       "11  aha south Tampa is getting flooded hag wait a second i live in south Tampa what am i gonna do what am i gonna do flick flooding     \n",
       "12  raining flooding Florida tamper Tampa 18 or 19 days vie lost count                                                                  \n",
       "13  flood in boga Myanmar we arrived boga                                                                                               \n",
       "14  damage to school bus on 80 in multi car crash breaking                                                                              \n",
       "15  whats up man                                                                                                                        \n",
       "16  i love fruits                                                                                                                       \n",
       "17  summer is lovely                                                                                                                    \n",
       "18  my car is so fast                                                                                                                   \n",
       "19  what a goooooooaaaaaal                                                                                                              "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "# looks like some text cleaning will be neccessary, keep it naive for now, but some ideas for later:\n",
    "# remove special charactes, puncuation, hashtags\n",
    "# fix spelling errors, remove extra letters\n",
    "tweets[['target','text']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data from the tweets\n",
    "train_x = np.asarray(tweets.text)\n",
    "# index all the sentiment labels\n",
    "train_y = np.asarray([[x] for x in tweets.target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to get serious\n",
    "import json\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only work with the 3000 most popular words found in our dataset\n",
    "max_words = 3000\n",
    "\n",
    "# create a new Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "# feed our tweets to the Tokenizer\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "\n",
    "#remove the top words, as these likely have no predictive power\n",
    "to_remove = list(dictionary.keys())[0:30]\n",
    "dictionary = dict([(k,v) for k,v in dictionary.items() if not k in to_remove])\n",
    "\n",
    "# Let's save this out so we can use it later\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)\n",
    "\n",
    "\n",
    "def convert_text_to_index_array(text):\n",
    "    # one really important thing that `text_to_word_sequence` does\n",
    "    # is make all texts the same length -- in this case, the length\n",
    "    # of the longest text in the set.\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text) if word not in to_remove]\n",
    "\n",
    "allWordIndices = []\n",
    "# for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "for text in train_x:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "# now we have a list of all tweets converted to index arrays.\n",
    "# cast as an array for future usage.\n",
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "# create one-hot matrices out of the indexed tweets\n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create our first neural network!\n",
    "if __name__ == \"__main__\":\n",
    "    X = train_x\n",
    "    y = train_y\n",
    "    nn = NeuralNetwork(X,y)\n",
    "\n",
    "    for i in range(1500):\n",
    "        nn.feedforward()\n",
    "        nn.backprop() \n",
    "    print(\"average error: \",sum((nn.output - y)**2)/len(y))\n",
    "\n",
    "pred = [int(x) for x in nn.output.round()]\n",
    "submission_1 = pd.DataFrame({\"id\":ids,\"target\":pred,'real':tweets.target})\n",
    "print(sum((submission_1.real - submission_1.target)**2)/len(submission_1.real))\n",
    "print(sum(submission_1.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like our model isn't working! It gives every prediction a score of 0 (i.e. not about disaster)\n",
    "\n",
    "There's lots of room for improvements, including:\n",
    "    1 raising the max word limt to > 3000 (didn't work)\n",
    "    2 cleaning the raw tweets, see above for ideas (didn't work\n",
    "    3 running more iterations (didn't work)\n",
    "\n",
    "Time to try something a bit more advanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "# treat the labels as categories\n",
    "train_y = keras.utils.to_categorical(train_y, 2)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "  optimizer='Adagrad',\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6851 samples, validate on 762 samples\n",
      "Epoch 1/5\n",
      "6851/6851 [==============================] - 5s 659us/step - loss: 0.5571 - acc: 0.7339 - val_loss: 0.4319 - val_acc: 0.7979\n",
      "Epoch 2/5\n",
      "6851/6851 [==============================] - 4s 552us/step - loss: 0.3590 - acc: 0.8491 - val_loss: 0.4355 - val_acc: 0.7913\n",
      "Epoch 3/5\n",
      "6851/6851 [==============================] - 4s 547us/step - loss: 0.2804 - acc: 0.8856 - val_loss: 0.4724 - val_acc: 0.7703\n",
      "Epoch 4/5\n",
      "6851/6851 [==============================] - 4s 555us/step - loss: 0.2251 - acc: 0.9096 - val_loss: 0.4860 - val_acc: 0.7874\n",
      "Epoch 5/5\n",
      "6851/6851 [==============================] - 4s 554us/step - loss: 0.1838 - acc: 0.9311 - val_loss: 0.5295 - val_acc: 0.7664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x284686dc188>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y,\n",
    "  batch_size=32,\n",
    "  epochs=5,\n",
    "  verbose=1,\n",
    "  validation_split=0.1,\n",
    "  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0,1]\n",
    "pred = model.predict(train_x)\n",
    "pred =  [labels[np.argmax(x)] for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      4342\n",
      "           1       0.94      0.92      0.93      3271\n",
      "\n",
      "    accuracy                           0.94      7613\n",
      "   macro avg       0.94      0.94      0.94      7613\n",
      "weighted avg       0.94      0.94      0.94      7613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score , recall_score, precision_score\n",
    "print(classification_report(tweets['target'], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open('model3.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model3.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
